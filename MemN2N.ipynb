{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MemN2N.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tjsiledar/Machine-Learning/blob/master/MemN2N.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2tnDNwt1ie5",
        "colab_type": "text"
      },
      "source": [
        "**Question Answering Model on the Facebook babI dataset using End to End Memory Network Model**\n",
        "\n",
        "\n",
        "The Facebook babI dataset consists of 20 tasks. We train our model on different tasks one by one and check the accuracies achieved. The trained models for each task are also provided with this file. Each is provided with the task number. For example. model5.h5 is for task 5 - Three Arg Relations.\n",
        "\n",
        "\n",
        "01. Single Supporting Fact - 96.30\n",
        "02. Two Supporting Facts - 27\n",
        "03. Three Supporting Facts - 20\n",
        "04. Two Arg Relations - 100\n",
        "05. Three Arg Relations - 86.60\n",
        "06. Yes/No questions - 80\n",
        "07. Counting - 77.40\n",
        "08. Lists/Sets - 73.80\n",
        "09. Simple Negation - 80.30\n",
        "10. Indefinite Knowledge - 94.40\n",
        "11. Basic Coreference - 99.20\n",
        "12. Conjunction - 97.90\n",
        "13. Compound Coreference   - 93.50\n",
        "14. Time Reasoning - 38.50\n",
        "15. Basic Deduction - 53.40\n",
        "16. Basic Induction - 44.80\n",
        "17. Positional Reasoning - 74\n",
        "18. Size Reasoning - 92.60\n",
        "19. Path Finding - 12.20\n",
        "20. Agent's Motivation - 97.8\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBjopz9pe2Gb",
        "colab_type": "code",
        "outputId": "b9024bc1-ec7f-483b-c4e3-2a7648916006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#libraries to download data and preprocess it.\n",
        "\n",
        "import re\n",
        "import tarfile\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMM4HjqxgAyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Function to create tokens of a sentence\n",
        "\n",
        "The input to the tokenize function is a sentence and it returns a list of tokens from the sentence\n",
        "\n",
        "a = \"hello how are you?\"\n",
        "tokenize(a)\n",
        "result --->  ['hello', 'how', 'are', 'you', '?']\n",
        "\n",
        "'''\n",
        "\n",
        "def tokenize(sentence):\n",
        "  return [x.strip() for x in re.split('(\\W+)?', sentence) if x.strip()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czi-Aev3jcRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Function to create tuples of each episode\n",
        "\n",
        "The input to the parse_episodes is a list of sentences and the output is a global data list consisting of tuple \n",
        "for each story in the form of (story, question, answer)\n",
        "'''\n",
        "\n",
        "def parse_episodes(lines):\n",
        "  \n",
        "  data = []\n",
        "  story = []\n",
        "  \n",
        "  for line in lines:\n",
        "    line = line.decode('utf-8').strip()\n",
        "    nid, line = line.split(' ', 1)\n",
        "    nid = int(nid)\n",
        "    \n",
        "    if nid==1:\n",
        "      \n",
        "      # id=1 means new story\n",
        "      story = []\n",
        "    \n",
        "    if '\\t' in line:\n",
        "      # here line is tab separated question, answer and supporting id.\n",
        "      q, a, support_id = line.split('\\t')\n",
        "      q = tokenize(q)\n",
        "      substory = [x for x in story if x]\n",
        "      \n",
        "      # add substory question and answer as a tuple to the data list.\n",
        "      # single story ends here and is added as a tuple to the global data list.\n",
        "      data.append((substory,q,a))\n",
        "      story.append('')\n",
        "      \n",
        "    else:\n",
        "      \n",
        "      # here each sentence is tokenized and added to the story list.\n",
        "      sentence = tokenize(line)\n",
        "      story.append(sentence)\n",
        "      \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19S-RAjm8pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Function to create a list of all episodes from the file\n",
        "'''\n",
        "\n",
        "def get_episodes(file):\n",
        "  \n",
        "  # file.readlines() returns all the sentences in the file in the form of one list and is then passed to our function parse_episodes.\n",
        "  data = parse_episodes(file.readlines())\n",
        "  \n",
        "  # flatten is defined to convert list of lists into a single list\n",
        "  flatten = lambda data: reduce(lambda x,y: x+y, data)\n",
        "  \n",
        "  data = [(flatten(story), question, answer) for story, question, answer in data]\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDS7q88qoAc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Function to vectorize episodes\n",
        "\n",
        "here we convert our stories, questions, answers into vector forms using a word-id dictionary created.\n",
        "'''\n",
        "\n",
        "def vectorize(data, word_idx, story_maxlen, query_maxlen):\n",
        "  stories, queries, answers = [], [], []\n",
        "  \n",
        "  for story, query, answer in data:\n",
        "    \n",
        "    # using word to id dictionary we map each word to a number so that our stories, question, and anwers are vectors consisting of numbers.\n",
        "    stories.append([word_idx[w] for w in story])\n",
        "    queries.append([word_idx[w] for w in query])\n",
        "    y = np.zeros(len(word_idx)+1)\n",
        "    y[word_idx[answer]]=1\n",
        "    answers.append(y)\n",
        "    \n",
        "  # pad_sequences are used to pad our vectors with zeros.\n",
        "  return (pad_sequences(stories, maxlen=story_maxlen), pad_sequences(queries, maxlen=query_maxlen), np.array(answers))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoSFG0XFfjW1",
        "colab_type": "code",
        "outputId": "cd426f08-1e3f-4862-96a9-4baa134b1315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#downloading the dataset\n",
        "\n",
        "path = get_file('babi-tasks-v1-2.tar.gz', origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
        "file = tarfile.open(path)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz\n",
            "11747328/11745123 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGzizQhhow05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset consists of 20 tasks from the Facebook babI dataset.\n",
        "\n",
        "dataset = {\n",
        "    \n",
        "    'single-supporting-fact' : 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
        "    'two-supporting-facts' : 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
        "    'three-supporting-facts' : 'tasks_1-20_v1-2/en-10k/qa3_three-supporting-facts_{}.txt',\n",
        "    'two-arg-relations' : 'tasks_1-20_v1-2/en-10k/qa4_two-arg-relations_{}.txt',\n",
        "    'three-arg-relations' : 'tasks_1-20_v1-2/en-10k/qa5_three-arg-relations_{}.txt',\n",
        "    'yes-no-questions' : 'tasks_1-20_v1-2/en-10k/qa6_yes-no-questions_{}.txt',\n",
        "    'counting' : 'tasks_1-20_v1-2/en-10k/qa7_counting_{}.txt',\n",
        "    'lists-sets' : 'tasks_1-20_v1-2/en-10k/qa8_lists-sets_{}.txt',\n",
        "    'simple-negation' : 'tasks_1-20_v1-2/en-10k/qa9_simple-negation_{}.txt',\n",
        "    'indefinite-knowledge' : 'tasks_1-20_v1-2/en-10k/qa10_indefinite-knowledge_{}.txt',\n",
        "    'basic-coreference' : 'tasks_1-20_v1-2/en-10k/qa11_basic-coreference_{}.txt',\n",
        "    'conjunction' : 'tasks_1-20_v1-2/en-10k/qa12_conjunction_{}.txt',\n",
        "    'compound-coreference' : 'tasks_1-20_v1-2/en-10k/qa13_compound-coreference_{}.txt',\n",
        "    'time-reasoning' : 'tasks_1-20_v1-2/en-10k/qa14_time-reasoning_{}.txt',\n",
        "    'basic-deduction' : 'tasks_1-20_v1-2/en-10k/qa15_basic-deduction_{}.txt',\n",
        "    'basic-induction' : 'tasks_1-20_v1-2/en-10k/qa16_basic-induction_{}.txt',\n",
        "    'positional-reasoning' : 'tasks_1-20_v1-2/en-10k/qa17_positional-reasoning_{}.txt',\n",
        "    'size-reasoning' : 'tasks_1-20_v1-2/en-10k/qa18_size-reasoning_{}.txt',\n",
        "    'path-finding' : 'tasks_1-20_v1-2/en-10k/qa19_path-finding_{}.txt',\n",
        "    'agents-motivations' : 'tasks_1-20_v1-2/en-10k/qa20_agents-motivations_{}.txt',\n",
        "    \n",
        "}\n",
        "\n",
        "current_dataset = 'three-arg-relations'\n",
        "\n",
        "dataset = dataset[current_dataset]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isoMK3BuprA1",
        "colab_type": "code",
        "outputId": "d2a8cf39-d6b5-499b-fdf4-487332b175f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Extracting train and test datasets\n",
        "\n",
        "train_set = get_episodes(file.extractfile(dataset.format('train')))\n",
        "test_set = get_episodes(file.extractfile(dataset.format('test')))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
            "  return _compile(pattern, flags).split(string, maxsplit)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX5ynx3fsBEq",
        "colab_type": "code",
        "outputId": "efca2d7b-8a82-471b-d921-d5950d7bc0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "#checking our sets\n",
        "\n",
        "print(len(train_set))\n",
        "print(len(test_set))\n",
        "\n",
        "print(train_set[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000\n",
            "1000\n",
            "(['Bill', 'travelled', 'to', 'the', 'office', '.', 'Bill', 'picked', 'up', 'the', 'football', 'there', '.', 'Bill', 'went', 'to', 'the', 'bedroom', '.', 'Bill', 'gave', 'the', 'football', 'to', 'Fred', '.'], ['What', 'did', 'Bill', 'give', 'to', 'Fred', '?'], 'football')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSIWeIBVxdxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating a vocabulary from our sentences and sorting it\n",
        "\n",
        "vocab = set()\n",
        "\n",
        "for story, query, answer in train_set + test_set:\n",
        "  vocab |= set(story + query + [answer])\n",
        "vocab = sorted(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYpsCTW0sfok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# as 0 is reserved for padding total vocab size +1\n",
        "\n",
        "vocab_size = len(vocab) + 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaJ80yncymVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#calculating the maximum length of story and query\n",
        "\n",
        "story_maxlen = max(map(len, (x for x,_,_ in train_set + test_set)))\n",
        "query_maxlen = max(map(len, (x for _,x,_ in train_set + test_set)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-3SkRKi1ZTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creating word to index and index to word dictionary\n",
        "\n",
        "word_idx = dict((c,i+1) for i,c in enumerate(vocab))\n",
        "idx_word = dict((i+1, c) for i,c in enumerate(vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAvI-jEs2Lc3",
        "colab_type": "code",
        "outputId": "b3d22967-804e-4678-e0e9-f3161c406efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(vocab_size)\n",
        "print(word_idx)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n",
            "{'.': 1, '?': 2, 'Bill': 3, 'Fred': 4, 'Jeff': 5, 'Mary': 6, 'What': 7, 'Who': 8, 'apple': 9, 'back': 10, 'bathroom': 11, 'bedroom': 12, 'did': 13, 'discarded': 14, 'down': 15, 'dropped': 16, 'football': 17, 'garden': 18, 'gave': 19, 'give': 20, 'got': 21, 'grabbed': 22, 'hallway': 23, 'handed': 24, 'journeyed': 25, 'kitchen': 26, 'left': 27, 'milk': 28, 'moved': 29, 'office': 30, 'passed': 31, 'picked': 32, 'put': 33, 'received': 34, 'the': 35, 'there': 36, 'to': 37, 'took': 38, 'travelled': 39, 'up': 40, 'went': 41}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs65UtGU2M8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vectorizing story, query and answer using vocab\n",
        "\n",
        "stories_train, queries_train, answers_train = vectorize(train_set, word_idx, story_maxlen, query_maxlen)\n",
        "stories_test, queries_test, answers_test = vectorize(test_set, word_idx, story_maxlen, query_maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXK7_UfT3Fsp",
        "colab_type": "code",
        "outputId": "3b2bf53b-4f16-492f-8850-073ace594772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "print(stories_train.shape)\n",
        "stories_train[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 782)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  3, 39, 37, 35, 30,  1,  3, 32, 40,\n",
              "       35, 17, 36,  1,  3, 41, 37, 35, 12,  1,  3, 19, 35, 17, 37,  4,  1],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGzeYo2k3XDE",
        "colab_type": "code",
        "outputId": "7554d064-c9e1-4bc9-e5e0-192c24130c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(queries_train.shape)\n",
        "queries_train[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 8)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  7, 13,  3, 20, 37,  4,  2], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxXtjcyI3Z8e",
        "colab_type": "code",
        "outputId": "3099e5a4-a2ba-46b9-8810-d567016fc4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(answers_train.shape)\n",
        "answers_train[0,:]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 42)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU0TpxNG7v5R",
        "colab_type": "text"
      },
      "source": [
        "Lets start buliding our End to End Memory Network model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf2kql1q3eZ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Permute, dot, add, concatenate\n",
        "from keras.layers import LSTM, Dense, Dropout, Input, Activation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej9R4I_a81HI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of epochs to run\n",
        "train_epochs = 100\n",
        "# Training batch size\n",
        "batch_size = 32\n",
        "# Hidden embedding size\n",
        "embed_size = 50\n",
        "# Number of nodes in LSTM layer\n",
        "lstm_size = 64\n",
        "# Dropout rate\n",
        "dropout_rate = 0.30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTlxUqbv83HF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "60f0ac8f-8deb-470f-f60f-f4a6b7afd6e3"
      },
      "source": [
        "#placeholders\n",
        "\n",
        "input_sequence = Input((story_maxlen,))\n",
        "question = Input((query_maxlen,))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0731 03:45:11.799602 139678408763264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0731 03:45:11.841530 139678408763264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ff4Zp9j0ZsnY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "d3744758-597d-4409-de44-ebaeec932fe9"
      },
      "source": [
        "#encoders\n",
        "\n",
        "input_encoder_m = Sequential()\n",
        "input_encoder_m.add(Embedding(input_dim=vocab_size, output_dim=embed_size))\n",
        "input_encoder_m.add(Dropout(dropout_rate))\n",
        "\n",
        "input_encoder_c = Sequential()\n",
        "input_encoder_c.add(Embedding(input_dim=vocab_size, output_dim=query_maxlen))\n",
        "input_encoder_c.add(Dropout(dropout_rate))\n",
        "\n",
        "question_encoder = Sequential()\n",
        "question_encoder.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=query_maxlen))\n",
        "question_encoder.add(Dropout(dropout_rate))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0731 03:45:12.465162 139678408763264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0731 03:45:12.484796 139678408763264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0731 03:45:12.495900 139678408763264 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpRBQcg6Zu-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_encoded_m = input_encoder_m(input_sequence)\n",
        "input_encoded_c = input_encoder_c(input_sequence)\n",
        "question_encoded = question_encoder(question)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx-t3ekUgK_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compute match between first input vector and question vector\n",
        "\n",
        "match = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\n",
        "match = Activation('softmax')(match)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AwrvXQWg0Xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add match to the second input vector\n",
        "\n",
        "res = add([match, input_encoded_c])\n",
        "res = Permute((2,1))(res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_qO-tUVlVL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concatenate the response vector with question vector\n",
        "\n",
        "answer = concatenate([res, question_encoded])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92o4rVzjlqlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answer = LSTM(lstm_size)(answer)\n",
        "answer = Dropout(dropout_rate)(answer)\n",
        "answer = Dense(vocab_size)(answer)\n",
        "answer = Activation('softmax')(answer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yKMu4HamJcB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f274499f-c46d-4c70-d86d-17798ee089c4"
      },
      "source": [
        "# building the model\n",
        "\n",
        "model = Model([input_sequence, question], answer)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0731 03:45:15.915228 139678408763264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0731 03:45:15.947654 139678408763264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndmRUKKVmq_c",
        "colab_type": "code",
        "outputId": "38b03a7e-1eb9-4c89-db70-b9e152ccf844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 782)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 8)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       multiple             2100        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_3 (Sequential)       (None, 8, 50)        2100        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 782, 8)       0           sequential_1[1][0]               \n",
            "                                                                 sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 782, 8)       0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "sequential_2 (Sequential)       multiple             336         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 782, 8)       0           activation_1[0][0]               \n",
            "                                                                 sequential_2[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "permute_1 (Permute)             (None, 8, 782)       0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 8, 832)       0           permute_1[0][0]                  \n",
            "                                                                 sequential_3[1][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 64)           229632      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 64)           0           lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 42)           2730        dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 42)           0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 236,898\n",
            "Trainable params: 236,898\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHdH4e8umuIG",
        "colab_type": "code",
        "outputId": "6b769e87-c8cf-4b97-ad4e-9702e630ebc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit([stories_train, queries_train], answers_train, batch_size, train_epochs, validation_data = ([stories_test, queries_test], answers_test))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0731 03:45:18.606018 139678408763264 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 10000 samples, validate on 1000 samples\n",
            "Epoch 1/100\n",
            "10000/10000 [==============================] - 14s 1ms/step - loss: 1.5872 - acc: 0.2646 - val_loss: 1.2976 - val_acc: 0.2870\n",
            "Epoch 2/100\n",
            "10000/10000 [==============================] - 10s 970us/step - loss: 1.2459 - acc: 0.3656 - val_loss: 1.0765 - val_acc: 0.4760\n",
            "Epoch 3/100\n",
            "10000/10000 [==============================] - 10s 959us/step - loss: 1.0799 - acc: 0.4675 - val_loss: 1.0077 - val_acc: 0.5090\n",
            "Epoch 4/100\n",
            "10000/10000 [==============================] - 10s 959us/step - loss: 0.9937 - acc: 0.5346 - val_loss: 0.9556 - val_acc: 0.5460\n",
            "Epoch 5/100\n",
            "10000/10000 [==============================] - 10s 952us/step - loss: 0.9436 - acc: 0.5628 - val_loss: 0.8957 - val_acc: 0.6050\n",
            "Epoch 6/100\n",
            "10000/10000 [==============================] - 9s 946us/step - loss: 0.8716 - acc: 0.6107 - val_loss: 0.8402 - val_acc: 0.6270\n",
            "Epoch 7/100\n",
            "10000/10000 [==============================] - 9s 947us/step - loss: 0.8085 - acc: 0.6476 - val_loss: 0.7761 - val_acc: 0.6590\n",
            "Epoch 8/100\n",
            "10000/10000 [==============================] - 9s 946us/step - loss: 0.7484 - acc: 0.6773 - val_loss: 0.7634 - val_acc: 0.6550\n",
            "Epoch 9/100\n",
            "10000/10000 [==============================] - 9s 946us/step - loss: 0.6764 - acc: 0.7145 - val_loss: 0.7779 - val_acc: 0.6610\n",
            "Epoch 10/100\n",
            "10000/10000 [==============================] - 9s 948us/step - loss: 0.6374 - acc: 0.7316 - val_loss: 0.6686 - val_acc: 0.7020\n",
            "Epoch 11/100\n",
            "10000/10000 [==============================] - 9s 946us/step - loss: 0.6059 - acc: 0.7470 - val_loss: 0.6421 - val_acc: 0.7140\n",
            "Epoch 12/100\n",
            "10000/10000 [==============================] - 9s 947us/step - loss: 0.5740 - acc: 0.7604 - val_loss: 0.6454 - val_acc: 0.7090\n",
            "Epoch 13/100\n",
            "10000/10000 [==============================] - 9s 943us/step - loss: 0.5379 - acc: 0.7774 - val_loss: 0.6850 - val_acc: 0.6990\n",
            "Epoch 14/100\n",
            "10000/10000 [==============================] - 9s 945us/step - loss: 0.4987 - acc: 0.7959 - val_loss: 0.5973 - val_acc: 0.7470\n",
            "Epoch 15/100\n",
            "10000/10000 [==============================] - 9s 949us/step - loss: 0.4702 - acc: 0.8141 - val_loss: 0.5694 - val_acc: 0.7640\n",
            "Epoch 16/100\n",
            "10000/10000 [==============================] - 9s 945us/step - loss: 0.4390 - acc: 0.8279 - val_loss: 0.5748 - val_acc: 0.7570\n",
            "Epoch 17/100\n",
            "10000/10000 [==============================] - 9s 947us/step - loss: 0.4152 - acc: 0.8385 - val_loss: 0.5290 - val_acc: 0.7850\n",
            "Epoch 18/100\n",
            "10000/10000 [==============================] - 9s 949us/step - loss: 0.3824 - acc: 0.8537 - val_loss: 0.5451 - val_acc: 0.8020\n",
            "Epoch 19/100\n",
            "10000/10000 [==============================] - 9s 946us/step - loss: 0.3600 - acc: 0.8648 - val_loss: 0.5145 - val_acc: 0.8080\n",
            "Epoch 20/100\n",
            "10000/10000 [==============================] - 9s 950us/step - loss: 0.3325 - acc: 0.8796 - val_loss: 0.5280 - val_acc: 0.8040\n",
            "Epoch 21/100\n",
            "10000/10000 [==============================] - 9s 950us/step - loss: 0.3184 - acc: 0.8851 - val_loss: 0.4969 - val_acc: 0.8230\n",
            "Epoch 22/100\n",
            "10000/10000 [==============================] - 9s 950us/step - loss: 0.2991 - acc: 0.8894 - val_loss: 0.5173 - val_acc: 0.8140\n",
            "Epoch 23/100\n",
            "10000/10000 [==============================] - 10s 957us/step - loss: 0.2820 - acc: 0.9005 - val_loss: 0.5038 - val_acc: 0.8230\n",
            "Epoch 24/100\n",
            "10000/10000 [==============================] - 10s 966us/step - loss: 0.2673 - acc: 0.9029 - val_loss: 0.4854 - val_acc: 0.8370\n",
            "Epoch 25/100\n",
            "10000/10000 [==============================] - 9s 950us/step - loss: 0.2507 - acc: 0.9110 - val_loss: 0.5442 - val_acc: 0.8210\n",
            "Epoch 26/100\n",
            "10000/10000 [==============================] - 10s 953us/step - loss: 0.2412 - acc: 0.9147 - val_loss: 0.5149 - val_acc: 0.8260\n",
            "Epoch 27/100\n",
            "10000/10000 [==============================] - 10s 954us/step - loss: 0.2188 - acc: 0.9221 - val_loss: 0.5287 - val_acc: 0.8320\n",
            "Epoch 28/100\n",
            "10000/10000 [==============================] - 10s 950us/step - loss: 0.2073 - acc: 0.9255 - val_loss: 0.5170 - val_acc: 0.8310\n",
            "Epoch 29/100\n",
            "10000/10000 [==============================] - 10s 955us/step - loss: 0.1998 - acc: 0.9301 - val_loss: 0.5447 - val_acc: 0.8350\n",
            "Epoch 30/100\n",
            "10000/10000 [==============================] - 10s 952us/step - loss: 0.1916 - acc: 0.9321 - val_loss: 0.5345 - val_acc: 0.8340\n",
            "Epoch 31/100\n",
            "10000/10000 [==============================] - 10s 952us/step - loss: 0.1903 - acc: 0.9320 - val_loss: 0.5279 - val_acc: 0.8370\n",
            "Epoch 32/100\n",
            "10000/10000 [==============================] - 10s 952us/step - loss: 0.1795 - acc: 0.9353 - val_loss: 0.5571 - val_acc: 0.8360\n",
            "Epoch 33/100\n",
            "10000/10000 [==============================] - 10s 951us/step - loss: 0.1634 - acc: 0.9427 - val_loss: 0.5819 - val_acc: 0.8310\n",
            "Epoch 34/100\n",
            "10000/10000 [==============================] - 9s 949us/step - loss: 0.1687 - acc: 0.9413 - val_loss: 0.5682 - val_acc: 0.8330\n",
            "Epoch 35/100\n",
            "10000/10000 [==============================] - 9s 946us/step - loss: 0.1523 - acc: 0.9450 - val_loss: 0.5496 - val_acc: 0.8370\n",
            "Epoch 36/100\n",
            "10000/10000 [==============================] - 10s 955us/step - loss: 0.1490 - acc: 0.9489 - val_loss: 0.5572 - val_acc: 0.8430\n",
            "Epoch 37/100\n",
            "10000/10000 [==============================] - 10s 951us/step - loss: 0.1405 - acc: 0.9501 - val_loss: 0.5852 - val_acc: 0.8380\n",
            "Epoch 38/100\n",
            "10000/10000 [==============================] - 10s 954us/step - loss: 0.1467 - acc: 0.9509 - val_loss: 0.6133 - val_acc: 0.8370\n",
            "Epoch 39/100\n",
            "10000/10000 [==============================] - 9s 948us/step - loss: 0.1381 - acc: 0.9519 - val_loss: 0.6216 - val_acc: 0.8360\n",
            "Epoch 40/100\n",
            "10000/10000 [==============================] - 9s 936us/step - loss: 0.1281 - acc: 0.9533 - val_loss: 0.6481 - val_acc: 0.8290\n",
            "Epoch 41/100\n",
            "10000/10000 [==============================] - 9s 937us/step - loss: 0.1315 - acc: 0.9541 - val_loss: 0.6432 - val_acc: 0.8260\n",
            "Epoch 42/100\n",
            "10000/10000 [==============================] - 9s 942us/step - loss: 0.1189 - acc: 0.9594 - val_loss: 0.6658 - val_acc: 0.8290\n",
            "Epoch 43/100\n",
            "10000/10000 [==============================] - 9s 942us/step - loss: 0.1212 - acc: 0.9579 - val_loss: 0.6749 - val_acc: 0.8300\n",
            "Epoch 44/100\n",
            "10000/10000 [==============================] - 9s 940us/step - loss: 0.1213 - acc: 0.9567 - val_loss: 0.6726 - val_acc: 0.8290\n",
            "Epoch 45/100\n",
            "10000/10000 [==============================] - 9s 938us/step - loss: 0.1187 - acc: 0.9590 - val_loss: 0.7173 - val_acc: 0.8260\n",
            "Epoch 46/100\n",
            "10000/10000 [==============================] - 9s 940us/step - loss: 0.1192 - acc: 0.9599 - val_loss: 0.6867 - val_acc: 0.8250\n",
            "Epoch 47/100\n",
            "10000/10000 [==============================] - 9s 943us/step - loss: 0.1051 - acc: 0.9645 - val_loss: 0.6993 - val_acc: 0.8330\n",
            "Epoch 48/100\n",
            "10000/10000 [==============================] - 9s 940us/step - loss: 0.1055 - acc: 0.9654 - val_loss: 0.7236 - val_acc: 0.8320\n",
            "Epoch 49/100\n",
            "10000/10000 [==============================] - 9s 938us/step - loss: 0.0967 - acc: 0.9691 - val_loss: 0.6857 - val_acc: 0.8360\n",
            "Epoch 50/100\n",
            "10000/10000 [==============================] - 9s 937us/step - loss: 0.1049 - acc: 0.9659 - val_loss: 0.6661 - val_acc: 0.8350\n",
            "Epoch 51/100\n",
            "10000/10000 [==============================] - 9s 936us/step - loss: 0.1020 - acc: 0.9682 - val_loss: 0.6812 - val_acc: 0.8360\n",
            "Epoch 52/100\n",
            "10000/10000 [==============================] - 9s 937us/step - loss: 0.1032 - acc: 0.9645 - val_loss: 0.7002 - val_acc: 0.8330\n",
            "Epoch 53/100\n",
            "10000/10000 [==============================] - 9s 941us/step - loss: 0.0990 - acc: 0.9656 - val_loss: 0.6737 - val_acc: 0.8360\n",
            "Epoch 54/100\n",
            "10000/10000 [==============================] - 9s 941us/step - loss: 0.0998 - acc: 0.9670 - val_loss: 0.7164 - val_acc: 0.8380\n",
            "Epoch 55/100\n",
            "10000/10000 [==============================] - 9s 945us/step - loss: 0.0940 - acc: 0.9666 - val_loss: 0.7326 - val_acc: 0.8400\n",
            "Epoch 56/100\n",
            "10000/10000 [==============================] - 10s 953us/step - loss: 0.0916 - acc: 0.9692 - val_loss: 0.6779 - val_acc: 0.8410\n",
            "Epoch 57/100\n",
            "10000/10000 [==============================] - 10s 957us/step - loss: 0.0843 - acc: 0.9710 - val_loss: 0.6981 - val_acc: 0.8340\n",
            "Epoch 58/100\n",
            "10000/10000 [==============================] - 9s 938us/step - loss: 0.0791 - acc: 0.9726 - val_loss: 0.7443 - val_acc: 0.8290\n",
            "Epoch 59/100\n",
            "10000/10000 [==============================] - 9s 939us/step - loss: 0.0887 - acc: 0.9707 - val_loss: 0.7229 - val_acc: 0.8350\n",
            "Epoch 60/100\n",
            "10000/10000 [==============================] - 9s 942us/step - loss: 0.0863 - acc: 0.9707 - val_loss: 0.6867 - val_acc: 0.8390\n",
            "Epoch 61/100\n",
            "10000/10000 [==============================] - 9s 940us/step - loss: 0.0916 - acc: 0.9689 - val_loss: 0.6934 - val_acc: 0.8300\n",
            "Epoch 62/100\n",
            "10000/10000 [==============================] - 9s 941us/step - loss: 0.0866 - acc: 0.9725 - val_loss: 0.6956 - val_acc: 0.8320\n",
            "Epoch 63/100\n",
            "10000/10000 [==============================] - 9s 947us/step - loss: 0.0827 - acc: 0.9704 - val_loss: 0.7381 - val_acc: 0.8350\n",
            "Epoch 64/100\n",
            "10000/10000 [==============================] - 9s 946us/step - loss: 0.0834 - acc: 0.9716 - val_loss: 0.7371 - val_acc: 0.8380\n",
            "Epoch 65/100\n",
            "10000/10000 [==============================] - 9s 942us/step - loss: 0.0783 - acc: 0.9732 - val_loss: 0.6884 - val_acc: 0.8440\n",
            "Epoch 66/100\n",
            "10000/10000 [==============================] - 9s 941us/step - loss: 0.0804 - acc: 0.9736 - val_loss: 0.7467 - val_acc: 0.8320\n",
            "Epoch 67/100\n",
            "10000/10000 [==============================] - 9s 934us/step - loss: 0.0807 - acc: 0.9717 - val_loss: 0.7025 - val_acc: 0.8320\n",
            "Epoch 68/100\n",
            "10000/10000 [==============================] - 9s 938us/step - loss: 0.0730 - acc: 0.9754 - val_loss: 0.7751 - val_acc: 0.8280\n",
            "Epoch 69/100\n",
            "10000/10000 [==============================] - 9s 945us/step - loss: 0.0706 - acc: 0.9776 - val_loss: 0.7730 - val_acc: 0.8230\n",
            "Epoch 70/100\n",
            "10000/10000 [==============================] - 9s 937us/step - loss: 0.0712 - acc: 0.9750 - val_loss: 0.7949 - val_acc: 0.8360\n",
            "Epoch 71/100\n",
            "10000/10000 [==============================] - 9s 933us/step - loss: 0.0755 - acc: 0.9748 - val_loss: 0.7541 - val_acc: 0.8260\n",
            "Epoch 72/100\n",
            "10000/10000 [==============================] - 9s 940us/step - loss: 0.0639 - acc: 0.9782 - val_loss: 0.8003 - val_acc: 0.8280\n",
            "Epoch 73/100\n",
            "10000/10000 [==============================] - 9s 938us/step - loss: 0.0632 - acc: 0.9785 - val_loss: 0.8149 - val_acc: 0.8280\n",
            "Epoch 74/100\n",
            "10000/10000 [==============================] - 9s 938us/step - loss: 0.0671 - acc: 0.9779 - val_loss: 0.8565 - val_acc: 0.8240\n",
            "Epoch 75/100\n",
            "10000/10000 [==============================] - 9s 938us/step - loss: 0.0659 - acc: 0.9779 - val_loss: 0.8772 - val_acc: 0.8230\n",
            "Epoch 76/100\n",
            "10000/10000 [==============================] - 9s 938us/step - loss: 0.0679 - acc: 0.9778 - val_loss: 0.8257 - val_acc: 0.8330\n",
            "Epoch 77/100\n",
            "10000/10000 [==============================] - 9s 935us/step - loss: 0.0609 - acc: 0.9778 - val_loss: 0.8039 - val_acc: 0.8360\n",
            "Epoch 78/100\n",
            "10000/10000 [==============================] - 9s 942us/step - loss: 0.0653 - acc: 0.9772 - val_loss: 0.8516 - val_acc: 0.8340\n",
            "Epoch 79/100\n",
            "10000/10000 [==============================] - 9s 947us/step - loss: 0.0646 - acc: 0.9784 - val_loss: 0.8542 - val_acc: 0.8320\n",
            "Epoch 80/100\n",
            "10000/10000 [==============================] - 9s 943us/step - loss: 0.0609 - acc: 0.9802 - val_loss: 0.8049 - val_acc: 0.8340\n",
            "Epoch 81/100\n",
            "10000/10000 [==============================] - 9s 936us/step - loss: 0.0663 - acc: 0.9792 - val_loss: 0.8204 - val_acc: 0.8350\n",
            "Epoch 82/100\n",
            "10000/10000 [==============================] - 9s 938us/step - loss: 0.0665 - acc: 0.9780 - val_loss: 0.8441 - val_acc: 0.8320\n",
            "Epoch 83/100\n",
            "10000/10000 [==============================] - 9s 939us/step - loss: 0.0629 - acc: 0.9781 - val_loss: 0.8764 - val_acc: 0.8240\n",
            "Epoch 84/100\n",
            "10000/10000 [==============================] - 9s 942us/step - loss: 0.0557 - acc: 0.9822 - val_loss: 0.9292 - val_acc: 0.8370\n",
            "Epoch 85/100\n",
            "10000/10000 [==============================] - 9s 936us/step - loss: 0.0634 - acc: 0.9794 - val_loss: 0.8465 - val_acc: 0.8270\n",
            "Epoch 86/100\n",
            "10000/10000 [==============================] - 9s 938us/step - loss: 0.0593 - acc: 0.9790 - val_loss: 0.8540 - val_acc: 0.8370\n",
            "Epoch 87/100\n",
            "10000/10000 [==============================] - 9s 937us/step - loss: 0.0647 - acc: 0.9795 - val_loss: 0.8267 - val_acc: 0.8380\n",
            "Epoch 88/100\n",
            "10000/10000 [==============================] - 9s 947us/step - loss: 0.0664 - acc: 0.9775 - val_loss: 0.8325 - val_acc: 0.8460\n",
            "Epoch 89/100\n",
            "10000/10000 [==============================] - 9s 945us/step - loss: 0.0538 - acc: 0.9811 - val_loss: 0.8502 - val_acc: 0.8300\n",
            "Epoch 90/100\n",
            "10000/10000 [==============================] - 9s 944us/step - loss: 0.0565 - acc: 0.9814 - val_loss: 0.8861 - val_acc: 0.8310\n",
            "Epoch 91/100\n",
            "10000/10000 [==============================] - 9s 937us/step - loss: 0.0674 - acc: 0.9793 - val_loss: 0.8309 - val_acc: 0.8350\n",
            "Epoch 92/100\n",
            "10000/10000 [==============================] - 9s 942us/step - loss: 0.0582 - acc: 0.9822 - val_loss: 0.8082 - val_acc: 0.8440\n",
            "Epoch 93/100\n",
            "10000/10000 [==============================] - 9s 944us/step - loss: 0.0495 - acc: 0.9854 - val_loss: 0.8524 - val_acc: 0.8380\n",
            "Epoch 94/100\n",
            "10000/10000 [==============================] - 9s 939us/step - loss: 0.0587 - acc: 0.9802 - val_loss: 0.8929 - val_acc: 0.8370\n",
            "Epoch 95/100\n",
            "10000/10000 [==============================] - 9s 942us/step - loss: 0.0576 - acc: 0.9826 - val_loss: 0.8695 - val_acc: 0.8450\n",
            "Epoch 96/100\n",
            "10000/10000 [==============================] - 9s 941us/step - loss: 0.0579 - acc: 0.9809 - val_loss: 0.8907 - val_acc: 0.8360\n",
            "Epoch 97/100\n",
            "10000/10000 [==============================] - 9s 938us/step - loss: 0.0548 - acc: 0.9821 - val_loss: 0.9125 - val_acc: 0.8310\n",
            "Epoch 98/100\n",
            "10000/10000 [==============================] - 9s 941us/step - loss: 0.0550 - acc: 0.9815 - val_loss: 0.9323 - val_acc: 0.8220\n",
            "Epoch 99/100\n",
            "10000/10000 [==============================] - 9s 944us/step - loss: 0.0613 - acc: 0.9815 - val_loss: 0.9290 - val_acc: 0.8310\n",
            "Epoch 100/100\n",
            "10000/10000 [==============================] - 9s 938us/step - loss: 0.0535 - acc: 0.9826 - val_loss: 0.8879 - val_acc: 0.8330\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0919c3c978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDna8lj9H4yg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#saving our model\n",
        "\n",
        "model.save('model5.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VgDylYNnYCl",
        "colab_type": "code",
        "outputId": "976c3d0b-c90a-493e-8561-5f1f99b7e5cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "# checking predictions on first 10 stories from our test set\n",
        "\n",
        "for i in range(0,10):\n",
        "  c_input = test_set[i]\n",
        "  \n",
        "  # vectorizing our test story\n",
        "  c_story, c_query, c_answer = vectorize([c_input], word_idx, story_maxlen, query_maxlen)\n",
        "  \n",
        "  # using our model to predict \n",
        "  c_prediction = model.predict([c_story, c_query])\n",
        "  c_prediction = idx_word[np.argmax(c_prediction)]\n",
        "  \n",
        "  #printing our output\n",
        "  print(' '.join(c_input[0]), ' '.join(c_input[1]))\n",
        "  print(\"Prediction \" + str(c_prediction) + \" | \" + \"Answer : \" + str(c_input[2]))\n",
        "  print(\"-----------------------------------------------------------------------------------------\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fred picked up the football there . Fred gave the football to Jeff . What did Fred give to Jeff ?\n",
            "Prediction football | Answer : football\n",
            "-----------------------------------------------------------------------------------------\n",
            "Fred picked up the football there . Fred gave the football to Jeff . Bill went back to the bathroom . Jeff grabbed the milk there . Who gave the football to Jeff ?\n",
            "Prediction Fred | Answer : Fred\n",
            "-----------------------------------------------------------------------------------------\n",
            "Fred picked up the football there . Fred gave the football to Jeff . Bill went back to the bathroom . Jeff grabbed the milk there . Jeff gave the football to Fred . Fred handed the football to Jeff . What did Fred give to Jeff ?\n",
            "Prediction football | Answer : football\n",
            "-----------------------------------------------------------------------------------------\n",
            "Fred picked up the football there . Fred gave the football to Jeff . Bill went back to the bathroom . Jeff grabbed the milk there . Jeff gave the football to Fred . Fred handed the football to Jeff . Jeff handed the football to Fred . Fred gave the football to Jeff . Who did Fred give the football to ?\n",
            "Prediction Jeff | Answer : Jeff\n",
            "-----------------------------------------------------------------------------------------\n",
            "Fred picked up the football there . Fred gave the football to Jeff . Bill went back to the bathroom . Jeff grabbed the milk there . Jeff gave the football to Fred . Fred handed the football to Jeff . Jeff handed the football to Fred . Fred gave the football to Jeff . Jeff gave the football to Fred . Jeff put down the milk . Who did Jeff give the football to ?\n",
            "Prediction Fred | Answer : Fred\n",
            "-----------------------------------------------------------------------------------------\n",
            "Mary moved to the hallway . Jeff moved to the garden . Jeff got the apple there . Mary journeyed to the kitchen . Fred travelled to the garden . Jeff gave the apple to Fred . Who gave the apple ?\n",
            "Prediction Jeff | Answer : Jeff\n",
            "-----------------------------------------------------------------------------------------\n",
            "Mary moved to the hallway . Jeff moved to the garden . Jeff got the apple there . Mary journeyed to the kitchen . Fred travelled to the garden . Jeff gave the apple to Fred . Fred travelled to the hallway . Fred handed the apple to Bill . Who received the apple ?\n",
            "Prediction Bill | Answer : Bill\n",
            "-----------------------------------------------------------------------------------------\n",
            "Mary moved to the hallway . Jeff moved to the garden . Jeff got the apple there . Mary journeyed to the kitchen . Fred travelled to the garden . Jeff gave the apple to Fred . Fred travelled to the hallway . Fred handed the apple to Bill . Bill handed the apple to Fred . Mary got the football there . What did Bill give to Fred ?\n",
            "Prediction apple | Answer : apple\n",
            "-----------------------------------------------------------------------------------------\n",
            "Mary moved to the hallway . Jeff moved to the garden . Jeff got the apple there . Mary journeyed to the kitchen . Fred travelled to the garden . Jeff gave the apple to Fred . Fred travelled to the hallway . Fred handed the apple to Bill . Bill handed the apple to Fred . Mary got the football there . Fred handed the apple to Bill . Bill gave the apple to Fred . What did Bill give to Fred ?\n",
            "Prediction apple | Answer : apple\n",
            "-----------------------------------------------------------------------------------------\n",
            "Mary moved to the hallway . Jeff moved to the garden . Jeff got the apple there . Mary journeyed to the kitchen . Fred travelled to the garden . Jeff gave the apple to Fred . Fred travelled to the hallway . Fred handed the apple to Bill . Bill handed the apple to Fred . Mary got the football there . Fred handed the apple to Bill . Bill gave the apple to Fred . Mary travelled to the bedroom . Mary took the milk there . Who received the apple ?\n",
            "Prediction Fred | Answer : Fred\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6LzM243USBi",
        "colab_type": "code",
        "outputId": "eb68f735-57b1-4061-87e6-2678dcd85045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "source": [
        "'''\n",
        "Make sure the story and query you input should contain spaces after every word. Even for fullstop and question mark\n",
        "\n",
        "The story and question should contain words only from our vocabulary. \n",
        "'''  \n",
        "while 1 :\n",
        "  print('Please input a story')\n",
        "  user_story_inp = input().split(' ')\n",
        "  print('Please input a query')\n",
        "  user_query_inp = input().split(' ')\n",
        "  user_story, user_query, user_ans = vectorize([[user_story_inp, user_query_inp, '.']], word_idx, story_maxlen, query_maxlen)\n",
        "  user_prediction = model.predict([user_story, user_query])\n",
        "  user_prediction = idx_word[np.argmax(user_prediction)]\n",
        "  print('Result')\n",
        "  print(' '.join(user_story_inp), ' '.join(user_query_inp))\n",
        "  print('| Prediction:', user_prediction)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please input a story\n",
            "Jeff travelled to the garden . Mary moved to the kitchen . Mary moved to the kitchen . Fred picked up the football there . Bill went to the bathroom . Fred gave the football to Mary . Bill travelled to the hallway .\n",
            "Please input a query\n",
            "Who received the football ?\n",
            "Result\n",
            "Jeff travelled to the garden . Mary moved to the kitchen . Mary moved to the kitchen . Fred picked up the football there . Bill went to the bathroom . Fred gave the football to Mary . Bill travelled to the hallway . Who received the football ?\n",
            "| Prediction: Mary\n",
            "Please input a story\n",
            "Jeff travelled to the garden . Mary moved to the kitchen . Mary moved to the kitchen . Fred picked up the football there . Bill went to the bathroom . Fred gave the football to Mary . Bill travelled to the hallway .\n",
            "Please input a query\n",
            "Who gave the football ?\n",
            "Result\n",
            "Jeff travelled to the garden . Mary moved to the kitchen . Mary moved to the kitchen . Fred picked up the football there . Bill went to the bathroom . Fred gave the football to Mary . Bill travelled to the hallway . Who gave the football ?\n",
            "| Prediction: Fred\n",
            "Please input a story\n",
            "Jeff travelled to the garden . Mary moved to the kitchen . Mary moved to the kitchen . Fred picked up the football there . Bill went to the bathroom . Fred gave the football to Mary . Bill travelled to the hallway .\n",
            "Please input a query\n",
            "What did Fred give to Mary ?\n",
            "Result\n",
            "Jeff travelled to the garden . Mary moved to the kitchen . Mary moved to the kitchen . Fred picked up the football there . Bill went to the bathroom . Fred gave the football to Mary . Bill travelled to the hallway . What did Fred give to Mary ?\n",
            "| Prediction: football\n",
            "Please input a story\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9tvtNYxctdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}